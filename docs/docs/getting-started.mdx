---
id: getting-started
title: Quick Introduction
sidebar_label: Quick Introduction
---

import Envelope from "@site/src/components/envelope";

**DeepEval** is an open-source evaluation framework for LLMs. DeepEval makes it extremely easy to build
and iterate on LLM (applications) and was built with the following principles in mind:

- Easily "unit test" LLM outputs in a similar way to Pytest.
- Plug-and-use 14+ LLM-evaluated metrics, most with research backing.
- Synthetic dataset generation with state-of-the-art evolution techniques.
- Metrics are simple to customize and covers all use cases.
- Red team, safety scan LLM applications for security vulnerabilities.
- Real-time evaluations in production.

Additionally, DeepEval integrates natively with [Confident AI](https://app.confident-ai.com), which allows anyone to **evaluate, regression test, red team, and monitor** LLM applications on the cloud.

<Envelope />

:::tip DID YOU KNOW?
**Confident AI is the LLM evaluation platform** built specifically for `deepeval`. It allows you to manage your entire LLM evaluation workflow (datasets, testing reports, monitoring, etc.) in one centralized place. It makes sure you do LLM evaluations the right way.

<img
  src="https://confident-docs.s3.us-east-1.amazonaws.com/overview-page.png"
  alt="Confident AI"
/>

**It takes no additional code to setup**, is automatically integrated with all code you run using `deepeval`, and you can [click here to sign up for free](https://app.confident-ai.com/auth/signup).
:::

## Setup A Python Environment

Go to the root directory of your project and create a virtual environment (if you don't already have one). In the CLI, run:

```console
python3 -m venv venv
source venv/bin/activate
```

## Installation

In your newly created virtual environment, run:

```console.log
pip install -U deepeval
```

`deepeval` runs evaluations locally on your enviornment. However, you can also **run evaluations directly on** [Confident AI](/confident-ai/confident-ai-introduction), the leading evaluation platform for DeepEval:

```console
deepeval login
```

:::info
**Confident AI is free to try**, and allows you to keep all evaluation results on the cloud. Sign up [here.](https://app.confident-ai.com)
:::

## Create Your First Test Case

Run `touch test_example.py` to create a test file in your root directory. Open `test_example.py` and paste in your first test case:

```python title="test_example.py"
from deepeval import assert_test
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric

def test_answer_relevancy():
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    test_case = LLMTestCase(
        input="What if these shoes don't fit?",
        # Replace this with the actual output of your LLM application
        actual_output="We offer a 30-day full refund at no extra cost.",
        retrieval_context=["All customers are eligible for a 30 day full refund at no extra cost."]
    )
    assert_test(test_case, [answer_relevancy_metric])
```

Run `deepeval test run` from the root directory of your project:

```console
deepeval test run test_example.py
```

**Congratulations! Your test case should have passed âœ…** Let's breakdown what happened.

- The variable `input` mimics a user input, and `actual_output` is a placeholder for what your application's supposed to output based on this input.
- The variable `retrieval_context` contains the retrieved context from your knowledge base, and `AnswerRelevancyMetric(threshold=0.5)` is an default metric provided by `deepeval` for you to evaluate your LLM output's relevancy based on the provided retrieval context.
- All metric scores range from 0 - 1, which the `threshold=0.5` threshold ultimately determines if your test have passed or not.

:::info
You'll need to set your `OPENAI_API_KEY` as an enviornment variable before running the `AnswerRelevancyMetric`, since the `AnswerRelevancyMetric` is an LLM-evaluated metric.

To use **ANY** custom LLM of your choice, [check out this part of the docs](/guides/guides-using-custom-llms).
:::

You can either **save test runs on Confident AI for analysis and comparison for a more robust evaluation workflow**, or locally on your machine.

#### Confident AI (recommended)

Simply login with `deepeval login` (or [click here](https://app.confident-ai.com)) to get your API key.

```console
deepeval login
```

After you've pasted in your API key, Confident AI will generate testing reports for you whenever you run a test run to evaluate your LLM application inside any environment, at any scale, anywhere.

<img
  src="https://confident-docs.s3.us-east-1.amazonaws.com/test-cases-page.png"
  alt="Confident AI"
  style={{
    marginTop: "20px",
    marginBottom: "40px",
    height: "auto",
    maxHeight: "500px",
  }}
/>

:::tip Did you know?
**You should also save your test run as a dataset on Confident AI**, which allows you to reuse the set of `input`s and any `expected_output`, `context`, etc. for subsequent evaluations.

This workflow allows you to run experiments with different models, prompts, and pinpoint regressions/improvements, and allows for domain experts to collaborate on evaluation datasets that is otherwise difficult for an engineer, researcher, and data scientist to curate. This is the "correct" way to do evaluations.
:::

#### Locally

Simply set the `DEEPEVAL_RESULTS_FOLDER` environment variable to your relative path of choice.

```console
# linux
export DEEPEVAL_RESULTS_FOLDER="./data"

# or windows
set DEEPEVAL_RESULTS_FOLDER=.\data
```

## Run Another Test Run

The whole point of evaluation is to help you iterate towards a better LLM application, and you can do this by comparing the results of two test runs. Simply run another evaluation (we'll use a different `actual_output` in this example):

```python title="test_example.py"
from deepeval import assert_test
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric

def test_answer_relevancy():
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    test_case = LLMTestCase(
        input="What if these shoes don't fit?",
        # Replace this with the actual output of your LLM application
        actual_output="I agree, its a pretty nice day today",
        retrieval_context=["All customers are eligible for a 30 day full refund at no extra cost."]
    )
    assert_test(test_case, [answer_relevancy_metric])
```

```console
deepeval test run test_example.py
```

:::info
In this example, we've delibrately showed a "worse" `actual_output` that is extremely irrelevant when compared to the previous test case. In reality, you'll not be hardcoding the `actual_output`s but rather be generating them at evaluation time.
:::

After running the new testrun , you should see the `AnswerRelevancyMetric()` failing, and it is because the `actual_output` is now not relevant at all to the `input`. This is actually known as a "regression", and you can catch these by including `deepeval` in CI/CD pipeliens, or just in a python script.

Based on these two results, you can decide which iterate is better, and whether the latest change you've made to your LLM application is safe to deploy.

### Comparing Iterations

Although you can go through hundreds of test cases in your terminal, here's what catching regressions/identifying improvements by comparing test runs looks like on [Confident AI (sign up here)](https://app.confident-ai.com):

<img
  src="https://confident-docs.s3.us-east-1.amazonaws.com/comparison-page.png"
  alt="Confident AI"
  style={{
    height: "auto",
    maxHeight: "500px",
  }}
/>

:::note
We didn't use the same test case data as shown above to demonstrate a more realistic example of what comparing two test runs looks like.
:::

If you look closely, you can see that for the same `LLMTestCase` (matched by [`name`](/docs/evaluation-test-cases#labeling-test-cases-for-confident-ai) or `input`), the difference in its `actual_output` led to a better `Correctness (GEval)` metric score.

Green rows mean your LLM improved on this particular test case, while red means it regressed. You'll want to look at the entire test run to see if your results are better or worse!

## Create Your First Metric

:::info
If you're having trouble deciding on which metric to use, you can follow [this tutorial](/tutorials/tutorial-metrics-defining-an-evaluation-criteria) or use [Confident AI's metrics recommendation tool](https://confident-ai.com).
:::

`deepeval` provides two types of LLM evaluation metrics to evaluate LLM outputs: plug-and-use **default** metrics, and **custom** metrics for any evaluation criteria.

### Default Metrics

`deepeval` offers 14+ research backed default metrics covering a wide range of use-cases (such as RAG and fine-tuning). To create a metric, simply import from the `deepeval.metrics` module:

```python
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric

test_case = LLMTestCase(input="...", actual_output="...")
relevancy_metric = AnswerRelevancyMetric(threshold=0.5)

relevancy_metric.measure(test_case)
print(relevancy_metric.score, relevancy_metric.reason)
```

Note that you can run a metric as a standalone or as part of a test run as shown in previous sections.

:::info
All default metrics are evaluated using LLMs, and you can use **ANY** LLM of your choice. For more information, visit the [metrics introduction section.](/docs/metrics-introduction)
:::

### Custom Metrics

`deepeval` provides G-Eval, a state-of-the-art LLM evaluation framework for anyone to create a custom LLM-evaluated metric using natural language. Here's an example:

```python
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import GEval

test_case = LLMTestCase(input="...", actual_output="...", expected_output="...")
correctness_metric = GEval(
    name="Correctness",
    criteria="Correctness - determine if the actual output is correct according to the expected output.",
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
    strict_mode=True
)

correctness_metric.measure(test_case)
print(correctness_metric.score, correctness_metric.reason)
```

Under the hood, `deepeval` first generates a series of evaluation steps, before using these steps in conjuction with information in an `LLMTestCase` for evaluation. For more information, visit the [G-Eval documentation page.](/docs/metrics-llm-evals)

:::tip
If you wish to customize your metrics a bit more, you can choose to implement your own metric. You can find [a comprehensive step-by-step guide here](/guides/guides-building-custom-metrics), but here's a quick example of how you can create a metric that is **NOT** evaluated using LLMs:

```python
from deepeval.scorer import Scorer
from deepeval.metrics import BaseMetric

class RougeMetric(BaseMetric):
    def __init__(self, threshold: float = 0.5):
        self.threshold = threshold
        self.scorer = Scorer()

    def measure(self, test_case: LLMTestCase):
        self.score = self.scorer.rouge_score(
            prediction=test_case.actual_output,
            target=test_case.expected_output,
            score_type="rouge1"
        )
        self.success = self.score >= self.threshold
        return self.score

    # Async implementation of measure(). If async version for
    # scoring method does not exist, just reuse the measure method.
    async def a_measure(self, test_case: LLMTestCase):
        return self.measure(test_case)

    def is_successful(self):
        return self.success

    @property
    def __name__(self):
        return "Rouge Metric"

#####################
### Example Usage ###
#####################
test_case = LLMTestCase(input="...", actual_output="...", expected_output="...")
metric = RougeMetric()

metric.measure(test_case)
print(metric.is_successful())
```

You'll notice that although not documented, `deepeval` additionally offers a `scorer` module for more traditional NLP scoring method and can be found [here.](https://github.com/confident-ai/deepeval/blob/main/deepeval/scorer/scorer.py)

You can also create a custom metric to combine several different metrics into one. For example. combining the `AnswerRelevancyMetric` and `FaithfulnessMetric` to test whether an LLM output is both relevant and faithful (ie. not hallucinating).

:::

## Measure Multiple Metrics At Once

To avoid redundant code, `deepeval` offers an easy way to apply as many metrics as you wish for a single test case.

```python title="test_example.py"
...

def test_everything():
    assert_test(test_case, [answer_relevancy_metric, correctness_metric])
```

In this scenario, `test_everything` only passes if all metrics are passing. Run `deepeval test run` again to see the results:

```console
deepeval test run test_example.py
```

:::info
`deepeval` optimizes evaluation speed by running all metrics for each test case concurrently.
:::

## Create Your First Dataset

A dataset in `deepeval`, or more specifically an evaluation dataset, is simply a collection of `LLMTestCases` and/or `Goldens`.

:::note
A `Golden` is simply an `LLMTestCase` with no `actual_output`, and it is an important concept if you're looking to generate LLM outputs at evaluation time. To learn more about `Golden`s, [click here.](/docs/evaluation-datasets#with-goldens)
:::

To create a dataset, simply initialize an `EvaluationDataset` with a list of `LLMTestCase`s or `Golden`s:

```python
from deepeval.test_case import LLMTestCase
from deepeval.dataset import EvaluationDataset

first_test_case = LLMTestCase(input="...", actual_output="...")
second_test_case = LLMTestCase(input="...", actual_output="...")

dataset = EvaluationDataset(test_cases=[first_test_case, second_test_case])
```

Then, using `deepeval`'s Pytest integration, you can utilize the `@pytest.mark.parametrize` decorator to loop through and evaluate your dataset.

```python title="test_dataset.py"
import pytest
from deepeval import assert_test
from deepeval.metrics import AnswerRelevancyMetric
...

# Loop through test cases using Pytest
@pytest.mark.parametrize(
    "test_case",
    dataset,
)
def test_customer_chatbot(test_case: LLMTestCase):
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    assert_test(test_case, [answer_relevancy_metric])
```

:::tip
You can also evaluate entire datasets without going through the CLI (if you're in a notebook environment):

```python
from deepeval import evaluate
...

evaluate(dataset, [answer_relevancy_metric])
```

:::

Additionally you can run test cases in parallel by using the optional `-n` flag followed by a number (that determines the number of processes that will be used) when executing `deepeval test run`:

```
deepeval test run test_dataset.py -n 2
```

Visit the [evaluation introduction section](/docs/evaluation-introduction#evaluating-with-pytest) to learn about the different types of flags you can use with the `deepeval test run` command.

### Editing Datasets

Especially for those working as part of a team, or have domain experts annotating datasets for you, it is best practice to keep your dataset somewhre as one source of truth. Your team can annotate datasets directly on [Confident AI (signup here)](https://app.confident-ai.com):

<img
  src="https://confident-docs.s3.us-east-1.amazonaws.com/dataset-editing-page.png"
  alt="Confident AI"
/>

You can then pull the dataset from the cloud to evaluate locally like how you would pull a Github repo.

```python
from deepeval.dataset import EvaluationDataset
from deepeval.metrics import AnswerRelevancyMetric

dataset = EvaluationDataset()
# supply your dataset alias
dataset.pull(alias="QA Dataset")

evaluate(dataset, metrics=[AnswerRelevancyMetric()])
```

And you're done! All results will also be available on Confident AI available for comparison and analysis.

## Generate Synthetic Datasets

`deepeval` offers a synthetic data generator that uses state-of-the-art evolution techniques to make synthetic (aka. AI generated) datasets realistic. This is especially helpful if you don't have a prepared evaluation dataset, as it will **help you generate the initiate testing data you need** to get up and running with evaluation.

:::caution
You should aim to manually inspect and edit any synthetic data where possible.
:::

Simply supply a list of local document paths to generate a synthetic dataset from your knowledge base.

```python
from deepeval.synthesizer import Synthesizer
from deepeval.dataset import EvaluationDataset

synthesizer = Synthesizer()
goldens = synthesizer.generate_goldens_from_docs(document_paths=['example.txt', 'example.docx', 'example.pdf'])

dataset = EvaluationDataset(goldens=goldens)
```

After you're done with generating, simply evaluate your dataset as shown above. Note that `deepeval`'s `Synthesizer` does **NOT** generate `actual_output`s for each golden. This is because `actual_output`s are meant to be generated by your LLM (application), not `deepeval`'s synthesizer.

[Visit the synthesizer section](/docs/synthesizer-introduction) to learn how to customize `deepeval`'s synthetic data generation capabilities to your needs.

:::note
Remember, a `Golden` is basically an `LLMTestCase` but with no `actual_output`.
:::

## Red Team Your LLM application

LLM red teaming refers to the process of attacking your LLM application to expose any safety risks it may have, including but not limited to vulnerabilities such as bias, racism, encouraging illegal actions, etc. It is an automated way to test for LLM safety by prompting it with adversarial attacks, which will be all taken care of by `deepeval`.

:::info
Red teaming is a different form of testing from what you've seen above because while standard LLM evaluation tests your LLM on its **intended functionality**, red teaming is meant to test your LLM application against, intentional, adversarial attacks from malicious users.
:::

Here's how you can **scan your LLM for vulnerabilities in a few lines of code** using `deepeval`'s `RedTeamer`, an extremely powerful tool to automatically scan for [40+ vulnerabilities](red-teaming-vulnerabilities). First instantiate a `RedTeamer` instance:

```python
from deepeval.red_teaming import RedTeamer

red_teamer = RedTeamer(
    # describe purpose of your LLM application
    target_purpose="Provide financial advice related to personal finance and market trends.",
    # supply system prompt template
    target_system_prompt="You are a financial assistant designed to help users with financial planning"
)
```

Then, supply the target LLM application you wish to scan:

```python
from deepeval.red_teaming import AttackEnhancement, Vulnerability
...

results = red_teamer.scan(
    # your target LLM of type DeepEvalBaseLLM
    target_model=TargetLLM(),
    attacks_per_vulnerability=5,
    vulnerabilities=[v for v in Vulnerability],
    attack_enhancements={
        AttackEnhancement.BASE64: 0.25,
        AttackEnhancement.GRAY_BOX_ATTACK: 0.25,
        AttackEnhancement.JAILBREAK_CRESCENDO: 0.25,
        AttackEnhancement.MULTILINGUAL: 0.25,
    },
)
print("Red Teaming Results: ", results)
```

`deepeval`'s `RedTeamer` is highly customizable and offers a range of different advanced red teaming capabilities for anyone to leverage. We highly recommend you read more about the `RedTeamer` at the [red teaming section.](/docs/red-teaming-introduction)

:::tip
The `TargetLLM()` you see being provided as argument to the `target_model` parameter is of type `DeepEvalBaseLLM`, which is basically a wrapper to wrap your LLM application into `deepeval`'s ecosystem for easy evaluating. You can learn how to create a custom `TargetLLM` in a few lines of code [here.](/guides/guides-using-custom-llms#creating-a-custom-llm)
:::

And that's it! You now know how to not only test your LLM application for its functionality, but also for any underlying risks and vulnerabilities it may expose and make your systems susceptible to malicious attacks.

## Using Confident AI

If you have reached this point, you've likely ran `deepeval test run` multiple times. To keep track of all future evaluation results created by `deepeval`, [login to Confident AI](https://app.confident-ai.com/auth/signup) by running the following command in the CLI:

```console
deepeval login
```

:::note
Follow the instructions displayed on the CLI to create an account, get your Confident API key, and paste it in the CLI. You should see a message congratulating your successful login.
:::

[Confident AI](https://www.confident-ai.com/) is an all-in-one platform that unlocks `deepeval`'s full potential, and allows anyone to easily:

- Run evaluations directly on Confident AI (instead of locally via `deepeval`)
- Keep track of and debug historical test run results
- Discover optimal hyperparameters, such as the best models and prompt templates to use
- Centralize and synthesize evaluation datasets on the cloud
- Safeguard against breaking changes in CI/CD pipelines
- Monitor and trace LLM applications
- Run real-time evaluations in production, with custom metrics

:::tip
[Click here](/confident-ai/confident-ai-introduction) for the full documentation on using Confident AI through `deepeval`.
:::

By logging in, you can:

- **Run evaluations directly on Confident AI's infrastructure (recommended)**
- Run evaluations locally via `deepeval` and have it sent to Confident AI upon completion

Both workflows would allow you to view, share, edit, and comment on resulting test runs on Confident AI.

:::info
If you do decide to stick to running evaluations locally instead, you'll be able to view test run results on Confident AI each time you execute a test run:

```python title="test_example.py"
from deepeval import assert_test
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric

def test_answer_relevancy():
    test_case = LLMTestCase(input="...", actual_output="...")
    assert_test(test_case, [AnswerRelevancyMetric()])
```

```console
deepeval test run test_example.py
```

Once evaluation has finished, the test run result will be automatically sent to Confident AI. You should now see a link being returned upon test completion, and you can paste it in your browser to view results.
:::

### Running Evaluations On Confident AI

Confident AI allows anyone to define (custom) LLM evaluation metrics and run evaluations scalably on the cloud. To begin, all you need to do is [create an experiment on Confident AI](/confident-ai/confident-ai-testing-n-evaluation-experiments#creating-an-experiment) and send over test cases with required fields such as `actual_output`s generated by your LLM application that you wish to be evaluated.

:::note
All compute and LLMs required for evaluation are provided by Confident AI.
:::

```python
from deepeval import confident_evaluate
from deepeval.test_case import LLMTestCase

confident_evaluate(
    experiment_name="Your Experiment Name",
    test_cases=[LLMTestCase(...)]
)
```

### Managing and Generating Datasets

`deepeval` allows you to push and pull datasets stored on Confident AI, which is similar to pushing and pulling a repo from GitHub. To push a dataset to Confident AI, create an `EvaluationDataset` instance, populate it with test cases, and use the `push()` method:

```python
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset(test_cases=[...])
dataset.push(alias="My First Dataset")
```

You can now edit, comment on, and manage test cases on the cloud instead of locally in a CSV file.

:::note
You can also create a dataset directly on Confident AI without going through `deepeval`.
:::

To pull the dataset for evaluation, use the `pull()` method and evaluate it as shown in previous sections:

```python
from deepeval import confident_evaluate
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset()
dataset.pull(alias="My First Dataset")

confident_evaluate(experiment_name="Your Experiment Name", dataset)
```

:::tip
In reality, you'll often times want to process the pulled dataset before evaluating it, since test cases in a dataset are stored as `Golden`s, which might not always be ready for evaluation (ie. missing an `actual_output`).

To see a concrete example and a more detailed explanation, visit the [evaluating datasets section.](/confident-ai/confident-ai-evaluation-dataset-evaluation)
:::

### Optimizing Hyperparameters

Confident AI helps you easily discover the optimal set of hyperparameters, which in `deepeval` refers to properties such as the models, prompt templates, etc. used when generating the `actual_output`s for each `LLMTestCase`.

:::caution
We're currently in the process of adding greater support for this feature but for now this section is only applicable if you're running evaluations locally using `deepeval`.
:::

To discover which set of hyperparameters gives you the best evaluation metrics results, use the `@deepeval.log_hyperparameters` decorator:

```python title="test_example.py"
import pytest
import deepeval
from deepeval import assert_test
from deepeval.dataset import EvaluationDataset
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric

dataset = EvaluationDataset()
dataset.pull(alias="My First Dataset")

# Loop through test cases using Pytest
@pytest.mark.parametrize(
    "test_case",
    dataset,
)
def test_customer_chatbot(test_case: LLMTestCase):
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    assert_test(test_case, [answer_relevancy_metric])


# You should aim to make these values dynamic
@deepeval.log_hyperparameters(model="gpt-4", prompt_template="...")
def hyperparameters():
    # Return a dict to log additional hyperparameters.
    # You can also return an empty dict {} if there's no additional parameters to log
    return {
        "temperature": 1,
        "chunk size": 500
    }
```

:::note
The `hyperparameters()` function **DOESN'T** necessarily have to be named 'hyperparameters'. All you need in order to log hyperparameters on Confident AI is to decorate a function that returns a valid dictionary.
:::

Once you've added this decorator, execute `test_example.py` once more:

```console
deepeval test run test_example.py
```

The `@deepeval.log_hyperparameters` decorator helps Confident AI keep track of the hyperparameters used when generating the `actual_output`s for a particular test run. This allows you to **identify which combination of hyperparameters gives the best evaluation metric results over time.**

:::tip
You can also log hyperparameters via the `evaluate()` function:

```python
from deepeval import evaluate
...

evaluate(
    test_cases=[...],
    metrics=[...],
    hyperparameters={"model": "gpt4o", "prompt template": "..."}
)
```

Feel free to execute this in a nested for loop to figure out the best hyperparameter combination!

:::

### Monitoring LLM Responses

Confident AI allows anyone to [monitor, trace, and evaluate LLM responses in real-time.](/confident-ai/confident-ai-llm-monitoring) A single API request is all that's required, and this would typically happen at your servers right before returning an LLM response to your users:

```python
import deepeval

# At the end of your LLM call
response_id = deepeval.monitor(
    event_name="Chatbot",
    model="gpt-4",
    input="Example input.",
    response="Example response.",
    retrieval_context=["..."]
)
```

Confident AI will automatically run evaluations for each incoming LLM response on metrics you have turned on. Simply head over to the 'Project Details' page on Confident AI to turn on these real-time metrics.

:::info
You can also trace LLM applications on Confident AI. Learn more about how to setup tracing [here.](/confident-ai/confident-ai-tracing)

<video width="100%" autoPlay loop muted playsInlines>
  <source
    src="https://confident-bucket.s3.amazonaws.com/tracing.mp4"
    type="video/mp4"
  />
</video>
:::

### Collecting User Feedback

Confident AI allows you to send human feedback on LLM responses monitored in production, all via one API call by using the previously returned `response_id` from `deepeval.monitor()`:

```python
import deepeval
...

deepeval.send_feedback(
    response_id=response_id,
    provider="user",
    rating=7,
    explanation="Although the response is accurate, I think the spacing makes it hard to read."
)
```

Confident AI allows you to keep track of either `"user"` feedback (ie. feedback provided by end users interacting with your LLM application), or `"reviewer"` feedback (ie. feedback provided by reviewers manually checking the quality of LLM responses in production).

:::note
To learn more, visit the [human feedback section page.](/confident-ai/confident-ai-human-feedback)
:::

## Full Example

You can find the full example [here on our Github](https://github.com/confident-ai/deepeval/blob/main/examples/getting_started/test_example.py).
